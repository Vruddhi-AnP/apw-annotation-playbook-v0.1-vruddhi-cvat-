Purpose:
To monitor annotation quality, track progress, and manage changes during execution.

Scope:
In: Quality checks, metrics tracking, change handling
Out: Final delivery and closure

Who runs this:
Annotation Lead / Reviewer

Inputs needed:
- Completed annotation batches
- Daily production reports

Outputs produced:
- QA reports
- Approved or rejected batches
- Logged change requests

Tools touched:
- Annotation tool (CVAT)
- Documentation

Timebox:
20–30 minutes per batch.

Failure modes:
- QA not performed
- Metrics not tracked
- Changes applied without approval

Escalation rules:
- If quality drops below acceptable level, pause the batch and inform the lead.

Assumption ledger:
- Review process is already defined in execution SOP.

Step-by-step procedure:
1. Review completed batches → check label accuracy.
2. Record QA findings → note errors and patterns.
3. Track basic metrics → items reviewed and issues found.
4. Handle change requests → log and approve before applying.
5. Update status → reflect changes in reports.

Quality checks:
- QA is performed on every batch.
- Changes are approved before execution.

Artifacts produced:
- TPL_QA_Report_v0.1.md
- TPL_Change_Request_v0.1.md
- TPL_Client_Status_Update_v0.1.md

Example run:
- One batch reviewed, issues logged, and changes approved.
